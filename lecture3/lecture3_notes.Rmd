---
title: "Summary Statistics & Exploratory Data Analysis"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Outline

* Introductions
* What is EDA
* Data science workflow


## What is EDA?

Summarizing data in a way that help you form models

https://en.wikipedia.org/wiki/Exploratory_data_analysis
    

    In statistics, exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task. Exploratory data analysis was promoted by John Tukey to encourage statisticians to explore the data, and possibly formulate hypotheses that could lead to new data collection and experiments. 
    

    EDA is different from initial data analysis (IDA),[1] which focuses more narrowly on checking assumptions required for model fitting and hypothesis testing, and handling missing values and making transformations of variables as needed. EDA encompasses IDA.

    Tukey's championing of EDA encouraged the development of statistical computing packages, especially S at Bell Labs. The S programming language inspired the systems 'S'-PLUS and R. This family of statistical-computing environments featured vastly improved dynamic visualization capabilities, which allowed statisticians to identify outliers, trends and patterns in data that merited further study.

--- 

## Who is John Tukey?

Brief history of bell labs, S, S-Plus, and R

Popularity of R over time

---

## Recap: What is EDA?

Generally speaking, exploratory data analysis is separate from data modelling. What's the differnce? Modelling involves making assumptions about the data and the data generating process, and then performing rigorous statistical tests to confirm or challenge those assumptions. Exploratory data analysis involves learning about the data and formulating new questions and hypothesis. That is, before you make assumptions about the data. The process of exploration should generate multiple working hypothesis that can later be confirmed with rigorous model and testing. 

Emphasize
* simple interpretable transformations
* visuzlize the data
* due dilligence, look at the data
* detective work

De-emphasize 
* significance testing
* model specification


You'll find random patterns and real relationships in the data. Suspend assumptions. Do your due dilligence of looking at the data. 


---

## Data Representation

Variable
Observation
Data Set

## Data Set Types

* Time series
* Cross Section
* Panel


## EDA Techniques

* Summary stats: reduce the data to one number
    * mean and standard deviation (not defined for which distributions?)
    * 5-number summary (extremes: min & max, median quartiles -- defined for all distributions)
* Tables: 
    * a two-way table (crosstabulation): for two factors, shows how a response varies at different levels 
* Visualizations
    * stem and leaf plot (back to back?)
    * Box plot: (visualize mean, quantiles, "outliers")
    * scatter plot
    * line plot
    * smoothed line
    * Counts: histogram with bin-ranges


## Cross Section

min, max, mean, mode, sd, var, skew, kurtosis

build a moments function?  

```{r moments}
E <- function(x, p = 1) {
    sum(x*p)/length(x)
}
moment <- function(x, n = 1, na.rm = FALSE) {
    result <- NA
    
    if ( na.rm == TRUE ) {
        result <- sum( (x - mean(x, na.rm = TRUE))^n, na.rm = na.rm ) /
                  (length(x) - sum(is.na(x)) - 1) 
    } else {
        result <- sum( (x - mean(x))^n ) / 
                  (length(x) - 1)
    }
    
    return(result)
}

x <- c(1:10, NA)
narm <- TRUE

var(x, na.rm = narm)
sd(x, na.rm = narm)^2
moment(x = x, n = 2, na.rm = narm)
```

## Time Series

decompose(timeseries)

moving average

lag

lead



## Panel